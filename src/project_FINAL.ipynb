{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions 1 is \"s\", -1 is \"b\"\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import *\n",
    "\n",
    "# save ind_delete to delete indices from test data\n",
    "tX, ind_delete = update_X(tX, bound_delete=0.8, bound_change=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_expand_data import *\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, y_train, X_test, y_test = split_data(tX, y, split_ratio=0.8)\n",
    "\n",
    "# store d to later use with real test set\n",
    "d=7\n",
    "\n",
    "# create expanded X_train and X_test and normalize\n",
    "X_train_poly, mu_train_poly, std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Least Squares GD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41122017806504824\n"
     ]
    }
   ],
   "source": [
    "from least_squares_GD import *\n",
    "\n",
    "initial_w = np.random.normal(0, 1e-1, X_train_poly.shape[1])\n",
    "max_iters = 300\n",
    "gamma = 0.008\n",
    "\n",
    "w, loss = least_squares_GD(y_train, X_train_poly, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9715214812097751\n"
     ]
    }
   ],
   "source": [
    "# I think this is shitty cause we have 72 params so needs param optimization\n",
    "\n",
    "from least_squares_SGD import *\n",
    "\n",
    "initial_w = np.random.normal(0, 1e-1, X_train_poly.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.001\n",
    "\n",
    "w, loss = least_squares_SGD(y_train, X_train_poly, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3228801100705618\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "\n",
    "w, loss_analytical = least_squares(y_train, X_train_poly)\n",
    "\n",
    "print(loss_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    N = tx.shape[0]\n",
    "    lambda_prim = 2 * N * lambda_\n",
    "    w = np.linalg.solve(tx.T@tx + lambda_prim * np.identity(tx.shape[1]), tx.T@y)\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    # ***************************************************\n",
    "    x_train, y_train, x_test, y_test = split_data(x.reshape(-1,1), y, ratio, seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    x_train_poly = expand_X(x_train.reshape(-1,1), degree)\n",
    "    x_test_poly = expand_X(x_test.reshape(-1,1), degree)\n",
    "\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        w = ridge_regression(y_train, x_train_poly, lambda_)[0]\n",
    "        rmse_tr.append(np.sqrt(2 * compute_loss_mse(y_train, x_train_poly, w)))\n",
    "        rmse_te.append(np.sqrt(2 * compute_loss_mse(y_test, x_test_poly, w)))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, Training RMSE=0.859, Testing RMSE=0.862\n",
      "lambda=0.000, Training RMSE=0.862, Testing RMSE=0.865\n",
      "lambda=0.000, Training RMSE=0.864, Testing RMSE=0.867\n",
      "lambda=0.001, Training RMSE=0.865, Testing RMSE=0.868\n",
      "lambda=0.001, Training RMSE=0.865, Testing RMSE=0.868\n",
      "lambda=0.001, Training RMSE=0.866, Testing RMSE=0.869\n",
      "lambda=0.001, Training RMSE=0.866, Testing RMSE=0.869\n",
      "lambda=0.001, Training RMSE=0.866, Testing RMSE=0.870\n",
      "lambda=0.002, Training RMSE=0.867, Testing RMSE=0.870\n",
      "lambda=0.002, Training RMSE=0.867, Testing RMSE=0.870\n",
      "lambda=0.002, Training RMSE=0.867, Testing RMSE=0.870\n",
      "lambda=0.002, Training RMSE=0.868, Testing RMSE=0.871\n",
      "lambda=0.002, Training RMSE=0.868, Testing RMSE=0.871\n",
      "lambda=0.003, Training RMSE=0.868, Testing RMSE=0.871\n",
      "lambda=0.003, Training RMSE=0.868, Testing RMSE=0.871\n",
      "lambda=0.003, Training RMSE=0.868, Testing RMSE=0.872\n",
      "lambda=0.003, Training RMSE=0.869, Testing RMSE=0.872\n",
      "lambda=0.003, Training RMSE=0.869, Testing RMSE=0.872\n",
      "lambda=0.004, Training RMSE=0.869, Testing RMSE=0.872\n",
      "lambda=0.004, Training RMSE=0.869, Testing RMSE=0.872\n",
      "lambda=0.004, Training RMSE=0.869, Testing RMSE=0.872\n",
      "lambda=0.004, Training RMSE=0.869, Testing RMSE=0.873\n",
      "lambda=0.004, Training RMSE=0.869, Testing RMSE=0.873\n",
      "lambda=0.005, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.005, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.005, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.005, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.005, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.006, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.006, Training RMSE=0.870, Testing RMSE=0.873\n",
      "lambda=0.006, Training RMSE=0.870, Testing RMSE=0.874\n",
      "lambda=0.006, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.006, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.007, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.007, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.007, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.007, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.007, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.008, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.008, Training RMSE=0.871, Testing RMSE=0.874\n",
      "lambda=0.008, Training RMSE=0.871, Testing RMSE=0.875\n",
      "lambda=0.008, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.008, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.009, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.009, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.009, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.009, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.010, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.010, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.010, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.010, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.010, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.011, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.011, Training RMSE=0.872, Testing RMSE=0.875\n",
      "lambda=0.011, Training RMSE=0.873, Testing RMSE=0.875\n",
      "lambda=0.011, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.011, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.012, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.012, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.012, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.012, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.012, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.013, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.013, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.013, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.013, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.013, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.014, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.014, Training RMSE=0.873, Testing RMSE=0.876\n",
      "lambda=0.014, Training RMSE=0.874, Testing RMSE=0.876\n",
      "lambda=0.014, Training RMSE=0.874, Testing RMSE=0.876\n",
      "lambda=0.014, Training RMSE=0.874, Testing RMSE=0.876\n",
      "lambda=0.015, Training RMSE=0.874, Testing RMSE=0.876\n",
      "lambda=0.015, Training RMSE=0.874, Testing RMSE=0.876\n",
      "lambda=0.015, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.015, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.015, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.016, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.016, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.016, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.016, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.016, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.017, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.017, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.017, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.017, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.017, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.018, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.018, Training RMSE=0.874, Testing RMSE=0.877\n",
      "lambda=0.018, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.018, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.018, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.019, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.019, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.019, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.019, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.019, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.020, Training RMSE=0.875, Testing RMSE=0.877\n",
      "lambda=0.020, Training RMSE=0.875, Testing RMSE=0.878\n",
      "lambda=0.020, Training RMSE=0.875, Testing RMSE=0.878\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.linspace(0.00001, 0.02, 100)\n",
    "\n",
    "ws = []\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        ws.append(ridge_regression(y_train, X_train_poly, lambda_)[0])\n",
    "        rmse_tr.append(np.sqrt(2 * compute_loss_mse(y_train, X_train_poly, ws[ind])))\n",
    "        rmse_te.append(np.sqrt(2 * compute_loss_mse(y_test, X_test_poly, ws[ind])))\n",
    "        print(\"lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)* Cross Validation (do we need it???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_indices(num_examples,k_fold):\n",
    "    \"\"\"\n",
    "    Splits data indices\n",
    "    num_examples: total samples in the dataset\n",
    "    k_fold: number fold of Cross Validation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array of shuffled indices with shape (k_fold, num_examples//k_fold)\n",
    "    \"\"\"\n",
    "    ind = np.arange(num_examples)\n",
    "    split_size = num_examples//k_fold\n",
    "    \n",
    "    # shuffle data\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    k_fold_indices = []\n",
    "    # Generate k_fold set of indices\n",
    "    k_fold_indices = [ind[k*split_size:(k+1)*split_size] for k in range(k_fold)]\n",
    "         \n",
    "    return np.array(k_fold_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions based on w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers\n",
    "\n",
    "p = predict_labels(w, X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 77.6 %\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %.1f %%' % (np.mean(p == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1e-05\n",
      "Train Accuracy: 72.8 %\n",
      "Lambda: 0.00021191919191919194\n",
      "Train Accuracy: 72.4 %\n",
      "Lambda: 0.0004138383838383839\n",
      "Train Accuracy: 72.3 %\n",
      "Lambda: 0.0006157575757575758\n",
      "Train Accuracy: 72.2 %\n",
      "Lambda: 0.0008176767676767678\n",
      "Train Accuracy: 72.1 %\n",
      "Lambda: 0.0010195959595959596\n",
      "Train Accuracy: 72.1 %\n",
      "Lambda: 0.0012215151515151516\n",
      "Train Accuracy: 72.1 %\n",
      "Lambda: 0.0014234343434343436\n",
      "Train Accuracy: 72.0 %\n",
      "Lambda: 0.0016253535353535355\n",
      "Train Accuracy: 72.0 %\n",
      "Lambda: 0.0018272727272727275\n",
      "Train Accuracy: 71.9 %\n",
      "Lambda: 0.0020291919191919193\n",
      "Train Accuracy: 71.9 %\n",
      "Lambda: 0.0022311111111111112\n",
      "Train Accuracy: 71.9 %\n",
      "Lambda: 0.002433030303030303\n",
      "Train Accuracy: 71.9 %\n",
      "Lambda: 0.002634949494949495\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.002836868686868687\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.003038787878787879\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.003240707070707071\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.003442626262626263\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.003644545454545455\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.003846464646464647\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.004048383838383838\n",
      "Train Accuracy: 71.8 %\n",
      "Lambda: 0.0042503030303030305\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.004452222222222222\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.004654141414141414\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.004856060606060606\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.005057979797979798\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.00525989898989899\n",
      "Train Accuracy: 71.7 %\n",
      "Lambda: 0.005461818181818182\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.005663737373737374\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.005865656565656566\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.006067575757575758\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.00626949494949495\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.006471414141414142\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.006673333333333333\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.006875252525252526\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.007077171717171717\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.0072790909090909095\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.007481010101010101\n",
      "Train Accuracy: 71.6 %\n",
      "Lambda: 0.0076829292929292935\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.007884848484848486\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.008086767676767677\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.008288686868686869\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.008490606060606061\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.008692525252525254\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.008894444444444444\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.009096363636363637\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.00929828282828283\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.009500202020202022\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.009702121212121212\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.009904040404040405\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.010105959595959597\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.010307878787878788\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.01050979797979798\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.010711717171717173\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.010913636363636365\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.011115555555555556\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.011317474747474748\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.01151939393939394\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.011721313131313133\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.011923232323232323\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.012125151515151516\n",
      "Train Accuracy: 71.5 %\n",
      "Lambda: 0.012327070707070708\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.0125289898989899\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.012730909090909091\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.012932828282828284\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.013134747474747476\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.013336666666666667\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.01353858585858586\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.013740505050505052\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.013942424242424244\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.014144343434343435\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.014346262626262627\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.01454818181818182\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.014750101010101012\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.014952020202020203\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.015153939393939395\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.015355858585858587\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.01555777777777778\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.015759696969696972\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.015961616161616163\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.016163535353535353\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.016365454545454548\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01656737373737374\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.01676929292929293\n",
      "Train Accuracy: 71.4 %\n",
      "Lambda: 0.016971212121212123\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.017173131313131314\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.017375050505050508\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.0175769696969697\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01777888888888889\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.017980808080808083\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.018182727272727274\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.018384646464646465\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01858656565656566\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01878848484848485\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.018990404040404044\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.019192323232323234\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.019394242424242425\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01959616161616162\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.01979808080808081\n",
      "Train Accuracy: 71.3 %\n",
      "Lambda: 0.02\n",
      "Train Accuracy: 71.3 %\n"
     ]
    }
   ],
   "source": [
    "# check for lambdas\n",
    "\n",
    "for i, w in enumerate(ws):\n",
    "    p = predict_labels(w, X_test_poly)\n",
    "    print(\"Lambda: {}\".format(lambdas[i]))\n",
    "    print('Train Accuracy: %.1f %%' % (np.mean(p == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.delete(tX_test, ind_delete, axis=1)\n",
    "\n",
    "tX_test_poly  = expand_X(tX_test,d)\n",
    "tX_test_poly[:,1:]  = (tX_test_poly[:,1:]-mu_train_poly)/std_train_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_analytical, tX_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
