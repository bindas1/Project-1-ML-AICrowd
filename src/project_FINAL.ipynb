{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions 1 is \"s\", -1 is \"b\"\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tX.shape)\n",
    "tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import *\n",
    "\n",
    "# save ind_delete to delete indices from test data\n",
    "tX, ind_delete = update_X(tX, bound_delete=0.8, bound_change=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_expand_data import *\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, y_train, X_test, y_test = split_data(tX, y, split_ratio=0.8)\n",
    "\n",
    "# store d to later use with real test set\n",
    "d=7\n",
    "\n",
    "# create expanded X_train and X_test and normalize\n",
    "X_train_poly, mu_train_poly, std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "X_test_poly  = expand_X(X_test,d)\n",
    "X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Least Squares GD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4072862969493159\n"
     ]
    }
   ],
   "source": [
    "from least_squares_GD import *\n",
    "\n",
    "initial_w = np.random.normal(0, 1e-1, X_train_poly.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "w, loss = least_squares_GD(y_train, X_train_poly, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8353725619951254\n"
     ]
    }
   ],
   "source": [
    "# I think this is shitty cause we have 72 params so needs param optimization\n",
    "\n",
    "from least_squares_SGD import *\n",
    "\n",
    "initial_w = np.random.normal(0, 1e-1, X_train_poly.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.001\n",
    "\n",
    "w, loss = least_squares_SGD(y_train, X_train_poly, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3228801100705618\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "\n",
    "w, loss_analytical = least_squares(y_train, X_train_poly)\n",
    "\n",
    "print(loss_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)* Cross Validation (do we need it???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_indices(num_examples,k_fold):\n",
    "    \"\"\"\n",
    "    Splits data indices\n",
    "    num_examples: total samples in the dataset\n",
    "    k_fold: number fold of Cross Validation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array of shuffled indices with shape (k_fold, num_examples//k_fold)\n",
    "    \"\"\"\n",
    "    ind = np.arange(num_examples)\n",
    "    split_size = num_examples//k_fold\n",
    "    \n",
    "    # shuffle data\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    k_fold_indices = []\n",
    "    # Generate k_fold set of indices\n",
    "    k_fold_indices = [ind[k*split_size:(k+1)*split_size] for k in range(k_fold)]\n",
    "         \n",
    "    return np.array(k_fold_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions based on w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05648531 -0.27337724  0.08573239  0.02571961  0.04590483  0.10738388\n",
      " -0.04931987 -0.02135578 -0.0106693  -0.02642069  0.04604404  0.16854969\n",
      " -0.14825516 -0.08293466  0.12381492 -0.01210349 -0.05104763  0.07434264\n",
      "  0.10201889 -0.06826561 -0.19442112  0.01377429  0.19278142 -0.13117964\n",
      " -0.0292591   0.08176608  0.10115333 -0.00234948  0.13309682 -0.01474767\n",
      "  0.10541429  0.1775804  -0.01838765 -0.09570489 -0.06882322 -0.01121606\n",
      " -0.08648079 -0.10723669  0.1013338   0.17430125  0.11995028 -0.03722523\n",
      " -0.16334417 -0.1229582   0.04695462 -0.13306874  0.06889762  0.05990705\n",
      "  0.0617199  -0.12687894 -0.05602627  0.02902963 -0.06000911  0.043653\n",
      " -0.01680583  0.00688197  0.3132869   0.04430124  0.0277345  -0.03170848\n",
      " -0.08332548  0.18111479  0.23710783 -0.09129975 -0.0480382   0.03098816\n",
      " -0.12784783 -0.06091947 -0.12412917 -0.1262007 ]\n",
      "(70,)\n",
      "Gradient Descent(0/499): loss=0.7616430706831929, w0=0.034209070029780235, w1=-0.09420164271377136\n",
      "Gradient Descent(1/499): loss=0.6503206148575301, w0=0.03071737932947944, w1=-0.09167003850422166\n",
      "Gradient Descent(2/499): loss=0.6043166271023683, w0=0.027260605536181464, w1=-0.089479217908898\n",
      "Gradient Descent(3/499): loss=0.5823976048823014, w0=0.02383839948081629, w1=-0.08751985304940578\n",
      "Gradient Descent(4/499): loss=0.569777609340713, w0=0.0204504154860047, w1=-0.08572469032529315\n",
      "Gradient Descent(5/499): loss=0.5610685561815235, w0=0.01709631133114091, w1=-0.08405171120402011\n",
      "Gradient Descent(6/499): loss=0.5542406694312152, w0=0.013775748217825625, w1=-0.08247409533230782\n",
      "Gradient Descent(7/499): loss=0.5484788931315506, w0=0.010488390735643385, w1=-0.08097423146992383\n",
      "Gradient Descent(8/499): loss=0.5434188235931179, w0=0.007233906828282805, w1=-0.0795401378856069\n",
      "Gradient Descent(9/499): loss=0.5388718404098514, w0=0.004011967759995658, w1=-0.07816331764176737\n",
      "Gradient Descent(10/499): loss=0.5347242770351357, w0=0.0008222480823912286, w1=-0.07683746897021393\n",
      "Gradient Descent(11/499): loss=0.5308989445485202, w0=-0.00233557439843727, w1=-0.07555770573588916\n",
      "Gradient Descent(12/499): loss=0.5273393318365966, w0=-0.005461818654457497, w1=-0.07432008263874525\n",
      "Gradient Descent(13/499): loss=0.5240023114712107, w0=-0.008556800467917703, w1=-0.07312130287485108\n",
      "Gradient Descent(14/499): loss=0.5208542317389225, w0=-0.011620832463243337, w1=-0.07195853539765606\n",
      "Gradient Descent(15/499): loss=0.5178685002718707, w0=-0.01465422413861592, w1=-0.0708292983261197\n",
      "Gradient Descent(16/499): loss=0.515023923942595, w0=-0.017657281897234815, w1=-0.06973138254818648\n",
      "Gradient Descent(17/499): loss=0.5123034941718488, w0=-0.020630309078267607, w1=-0.06866279998899384\n",
      "Gradient Descent(18/499): loss=0.5096934682149838, w0=-0.02357360598749015, w1=-0.06762174722171224\n",
      "Gradient Descent(19/499): loss=0.5071826628605031, w0=-0.026487469927620574, w1=-0.06660657880104214\n",
      "Gradient Descent(20/499): loss=0.5047619070974296, w0=-0.029372195228349725, w1=-0.0656157869098324\n",
      "Gradient Descent(21/499): loss=0.5024236162707132, w0=-0.03222807327607163, w1=-0.06464798523160296\n",
      "Gradient Descent(22/499): loss=0.5001614600175117, w0=-0.03505539254331643, w1=-0.0637018957550058\n",
      "Gradient Descent(23/499): loss=0.4979701029437761, w0=-0.03785443861788887, w1=-0.06277633769407548\n",
      "Gradient Descent(24/499): loss=0.4958450018544332, w0=-0.04062549423171555, w1=-0.06187021799764013\n",
      "Gradient Descent(25/499): loss=0.4937822470081863, w0=-0.043368839289404264, w1=-0.06098252309818001\n",
      "Gradient Descent(26/499): loss=0.4917784376710811, w0=-0.046084750896516036, w1=-0.060112311659818146\n",
      "Gradient Descent(27/499): loss=0.4898305844083612, w0=-0.048773503387556746, w1=-0.059258708153884516\n",
      "Gradient Descent(28/499): loss=0.48793603223300996, w0=-0.05143536835368707, w1=-0.05842089713465232\n",
      "Gradient Descent(29/499): loss=0.4860924000334127, w0=-0.05407061467015621, w1=-0.05759811811699277\n",
      "Gradient Descent(30/499): loss=0.48429753271621057, w0=-0.05667950852346072, w1=-0.05678966097758075\n",
      "Gradient Descent(31/499): loss=0.4825494632887577, w0=-0.05926231343823225, w1=-0.05599486181536532\n",
      "Gradient Descent(32/499): loss=0.4808463827188275, w0=-0.061819290303856154, w1=-0.05521309921738756\n",
      "Gradient Descent(33/499): loss=0.47918661588635353, w0=-0.06435069740082384, w1=-0.054443790883957545\n",
      "Gradient Descent(34/499): loss=0.47756860231327874, w0=-0.06685679042682187, w1=-0.05368639057347722\n",
      "Gradient Descent(35/499): loss=0.4759908806466111, w0=-0.06933782252256002, w1=-0.05294038533230678\n",
      "Gradient Descent(36/499): loss=0.4744520760947454, w0=-0.07179404429734078, w1=-0.05220529297933253\n",
      "Gradient Descent(37/499): loss=0.4729508901923184, w0=-0.07422570385437384, w1=-0.05148065981850761\n",
      "Gradient Descent(38/499): loss=0.47148609240530853, w0=-0.07663304681583663, w1=-0.05076605855574198\n",
      "Gradient Descent(39/499): loss=0.4700565131944142, w0=-0.07901631634768487, w1=-0.050061086399212244\n",
      "Gradient Descent(40/499): loss=0.4686610382375968, w0=-0.08137575318421467, w1=-0.04936536332451243\n",
      "Gradient Descent(41/499): loss=0.467298603577295, w0=-0.08371159565237923, w1=-0.04867853048813148\n",
      "Gradient Descent(42/499): loss=0.46596819150821, w0=-0.08602407969586219, w1=-0.04800024877455794\n",
      "Gradient Descent(43/499): loss=0.4646688270609076, w0=-0.08831343889891033, w1=-0.04733019746391576\n",
      "Gradient Descent(44/499): loss=0.4633995749672049, w0=-0.09057990450992805, w1=-0.04666807300845046\n",
      "Gradient Descent(45/499): loss=0.4621595370173306, w0=-0.09282370546483566, w1=-0.04601358790743913\n",
      "Gradient Descent(46/499): loss=0.460947849737632, w0=-0.09504506841019422, w1=-0.04536646967120758\n",
      "Gradient Descent(47/499): loss=0.4597636823323146, w0=-0.09724421772609924, w1=-0.0447264598659231\n",
      "Gradient Descent(48/499): loss=0.45860623484423363, w0=-0.09942137554884527, w1=-0.044093313231704706\n",
      "Gradient Descent(49/499): loss=0.45747473649881554, w0=-0.10157676179336392, w1=-0.04346679686736877\n",
      "Gradient Descent(50/499): loss=0.4563684442022962, w0=-0.10371059417543745, w1=-0.04284668947581732\n",
      "Gradient Descent(51/499): loss=0.45528664117108, w0=-0.10582308823369017, w1=-0.042232780664689086\n",
      "Gradient Descent(52/499): loss=0.45422863567343547, w0=-0.10791445735136049, w1=-0.041624870297438644\n",
      "Gradient Descent(53/499): loss=0.45319375986825405, w0=-0.10998491277785409, w1=-0.041022767890494534\n",
      "Gradient Descent(54/499): loss=0.4521813687283646, w0=-0.11203466365008283, w1=-0.04042629205257932\n",
      "Gradient Descent(55/499): loss=0.45119083903810364, w0=-0.11406391701358928, w1=-0.03983526996266026\n",
      "Gradient Descent(56/499): loss=0.4502215684566026, w0=-0.11607287784346075, w1=-0.039249536883342886\n",
      "Gradient Descent(57/499): loss=0.44927297463966614, w0=-0.11806174906503353, w1=-0.038668935706826786\n",
      "Gradient Descent(58/499): loss=0.44834449441423896, w0=-0.1200307315743906, w1=-0.03809331653081685\n",
      "Gradient Descent(59/499): loss=0.4474355830003842, w0=-0.12198002425865419, w1=-0.037522536262028845\n",
      "Gradient Descent(60/499): loss=0.44654571327643267, w0=-0.12390982401607512, w1=-0.03695645824514697\n",
      "Gradient Descent(61/499): loss=0.44567437508356605, w0=-0.12582032577592184, w1=-0.036394951915288194\n",
      "Gradient Descent(62/499): loss=0.4448210745666048, w0=-0.12771172251817017, w1=-0.035837892472203675\n",
      "Gradient Descent(63/499): loss=0.4439853335481634, w0=-0.12958420529299608, w1=-0.03528516057460624\n",
      "Gradient Descent(64/499): loss=0.4431666889336983, w0=-0.13143796324007373, w1=-0.03473664205315432\n",
      "Gradient Descent(65/499): loss=0.44236469214523705, w0=-0.13327318360768065, w1=-0.03419222764075101\n",
      "Gradient Descent(66/499): loss=0.4415789085818347, w0=-0.13509005177161162, w1=-0.0336518127189314\n",
      "Gradient Descent(67/499): loss=0.4408089171049941, w0=-0.13688875125390332, w1=-0.03311529707921494\n",
      "Gradient Descent(68/499): loss=0.4400543095474684, w0=-0.13866946374137207, w1=-0.03258258469839346\n",
      "Gradient Descent(69/499): loss=0.43931469024400854, w0=-0.14043236910396617, w1=-0.032053583526809246\n",
      "Gradient Descent(70/499): loss=0.4385896755827499, w0=-0.14217764541293437, w1=-0.03152820528875434\n",
      "Gradient Descent(71/499): loss=0.4378788935760433, w0=-0.1439054689588129, w1=-0.031006365294191207\n",
      "Gradient Descent(72/499): loss=0.437181983449635, w0=-0.14561601426923265, w1=-0.030487982261057544\n",
      "Gradient Descent(73/499): loss=0.4364985952491879, w0=-0.14730945412654825, w1=-0.029972978147474776\n",
      "Gradient Descent(74/499): loss=0.4358283894632094, w0=-0.14898595958529082, w1=-0.029461277993231786\n",
      "Gradient Descent(75/499): loss=0.43517103666152906, w0=-0.15064569998944596, w1=-0.028952809769962007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(76/499): loss=0.4345262171485151, w0=-0.1522888429895596, w1=-0.028447504239475296\n",
      "Gradient Descent(77/499): loss=0.4338936206302959, w0=-0.15391555455967212, w1=-0.027945294819744955\n",
      "Gradient Descent(78/499): loss=0.43327294589528, w0=-0.15552599901408354, w1=-0.02744611745808583\n",
      "Gradient Descent(79/499): loss=0.43266390050733067, w0=-0.1571203390239508, w1=-0.02694991051109249\n",
      "Gradient Descent(80/499): loss=0.4320662005109854, w0=-0.1586987356337195, w1=-0.026456614630936154\n",
      "Gradient Descent(81/499): loss=0.4314795701481471, w0=-0.16026134827739058, w1=-0.025966172657646693\n",
      "Gradient Descent(82/499): loss=0.4309037415857139, w0=-0.16180833479462492, w1=-0.025478529517031133\n",
      "Gradient Descent(83/499): loss=0.43033845465364007, w0=-0.16333985144668695, w1=-0.024993632123903507\n",
      "Gradient Descent(84/499): loss=0.42978345659295797, w0=-0.16485605293222835, w1=-0.024511429290322136\n",
      "Gradient Descent(85/499): loss=0.4292385018133052, w0=-0.1663570924029144, w1=-0.024031871638550275\n",
      "Gradient Descent(86/499): loss=0.4287033516595451, w0=-0.16784312147889363, w1=-0.023554911518474126\n",
      "Gradient Descent(87/499): loss=0.42817777418706976, w0=-0.16931429026411307, w1=-0.023080502929229318\n",
      "Gradient Descent(88/499): loss=0.4276615439454175, w0=-0.17077074736148037, w1=-0.022608601444802225\n",
      "Gradient Descent(89/499): loss=0.4271544417698414, w0=-0.17221263988787402, w1=-0.022139164143387298\n",
      "Gradient Descent(90/499): loss=0.42665625458049006, w0=-0.1736401134890038, w1=-0.02167214954029467\n",
      "Gradient Descent(91/499): loss=0.42616677518888385, w0=-0.1750533123541223, w1=-0.021207517524215052\n",
      "Gradient Descent(92/499): loss=0.4256858021113757, w0=-0.17645237923058962, w1=-0.020745229296660245\n",
      "Gradient Descent(93/499): loss=0.42521313938930994, w0=-0.17783745543829232, w1=-0.02028524731440861\n",
      "Gradient Descent(94/499): loss=0.4247485964156047, w0=-0.17920868088391798, w1=-0.019827535234794733\n",
      "Gradient Descent(95/499): loss=0.4242919877674913, w0=-0.18056619407508742, w1=-0.01937205786369202\n",
      "Gradient Descent(96/499): loss=0.423843133045168, w0=-0.18191013213434523, w1=-0.01891878110604562\n",
      "Gradient Descent(97/499): loss=0.42340185671612396, w0=-0.18324063081301048, w1=-0.01846767191882122\n",
      "Gradient Descent(98/499): loss=0.4229679879649109, w0=-0.18455782450488906, w1=-0.0180186982662431\n",
      "Gradient Descent(99/499): loss=0.4225413605481485, w0=-0.1858618462598489, w1=-0.017571829077201766\n",
      "Gradient Descent(100/499): loss=0.42212181265455045, w0=-0.18715282779725914, w1=-0.01712703420471837\n",
      "Gradient Descent(101/499): loss=0.42170918676978414, w0=-0.18843089951929537, w1=-0.016684284387359253\n",
      "Gradient Descent(102/499): loss=0.4213033295459687, w0=-0.1896961905241112, w1=-0.016243551212500085\n",
      "Gradient Descent(103/499): loss=0.4209040916756372, w0=-0.19094882861887896, w1=-0.015804807081344276\n",
      "Gradient Descent(104/499): loss=0.42051132776998784, w0=-0.192188940332699, w1=-0.015368025175605968\n",
      "Gradient Descent(105/499): loss=0.42012489624126514, w0=-0.19341665092938093, w1=-0.014933179425772426\n",
      "Gradient Descent(106/499): loss=0.41974465918910925, w0=-0.19463208442009605, w1=-0.014500244480865428\n",
      "Gradient Descent(107/499): loss=0.4193704822907265, w0=-0.19583536357590403, w1=-0.01406919567962567\n",
      "Gradient Descent(108/499): loss=0.4190022346947392, w0=-0.19702660994015395, w1=-0.013640009023048086\n",
      "Gradient Descent(109/499): loss=0.41863978891857223, w0=-0.19820594384076137, w1=-0.01321266114819999\n",
      "Gradient Descent(110/499): loss=0.41828302074925067, w0=-0.19937348440236274, w1=-0.012787129303257457\n",
      "Gradient Descent(111/499): loss=0.41793180914747774, w0=-0.20052934955834809, w1=-0.012363391323698916\n",
      "Gradient Descent(112/499): loss=0.41758603615487555, w0=-0.20167365606277363, w1=-0.011941425609598008\n",
      "Gradient Descent(113/499): loss=0.41724558680427004, w0=-0.20280651950215495, w1=-0.011521211103960875\n",
      "Gradient Descent(114/499): loss=0.4169103490329107, w0=-0.2039280543071425, w1=-0.011102727272055964\n",
      "Gradient Descent(115/499): loss=0.41658021359851843, w0=-0.2050383737640802, w1=-0.01068595408168706\n",
      "Gradient Descent(116/499): loss=0.41625507399805817, w0=-0.20613759002644857, w1=-0.010270871984362826\n",
      "Gradient Descent(117/499): loss=0.4159348263891383, w0=-0.20722581412619326, w1=-0.009857461897318705\n",
      "Gradient Descent(118/499): loss=0.4156193695139447, w0=-0.2083031559849405, w1=-0.009445705186349107\n",
      "Gradient Descent(119/499): loss=0.41530860462561453, w0=-0.20936972442510032, w1=-0.009035583649410091\n",
      "Gradient Descent(120/499): loss=0.4150024354169683, w0=-0.21042562718085853, w1=-0.008627079500954878\n",
      "Gradient Descent(121/499): loss=0.4147007679515108, w0=-0.21147097090905922, w1=-0.008220175356966297\n",
      "Gradient Descent(122/499): loss=0.4144035105966263, w0=-0.2125058611999779, w1=-0.007814854220652201\n",
      "Gradient Descent(123/499): loss=0.4141105739588867, w0=-0.21353040258798742, w1=-0.00741109946877165\n",
      "Gradient Descent(124/499): loss=0.41382187082140043, w0=-0.21454469856211686, w1=-0.00700889483856125\n",
      "Gradient Descent(125/499): loss=0.41353731608313055, w0=-0.21554885157650505, w1=-0.00660822441523258\n",
      "Gradient Descent(126/499): loss=0.4132568267001125, w0=-0.21654296306074935, w1=-0.006209072620013199\n",
      "Gradient Descent(127/499): loss=0.412980321628508, w0=-0.21752713343015123, w1=-0.005811424198705053\n",
      "Gradient Descent(128/499): loss=0.41270772176942766, w0=-0.2185014620958591, w1=-0.005415264210735381\n",
      "Gradient Descent(129/499): loss=0.4124389499154668, w0=-0.21946604747490991, w1=-0.005020578018676665\n",
      "Gradient Descent(130/499): loss=0.4121739306988895, w0=-0.22042098700017024, w1=-0.00462735127821308\n",
      "Gradient Descent(131/499): loss=0.41191259054141033, w0=-0.221366377130178, w1=-0.00423556992853228\n",
      "Gradient Descent(132/499): loss=0.4116548576055142, w0=-0.2223023133588857, w1=-0.003845220183122273\n",
      "Gradient Descent(133/499): loss=0.41140066174726614, w0=-0.22322889022530634, w1=-0.0034562885209542004\n",
      "Gradient Descent(134/499): loss=0.41114993447055787, w0=-0.2241462013230628, w1=-0.0030687616780327885\n",
      "Gradient Descent(135/499): loss=0.41090260888274405, w0=-0.2250543393098417, w1=-0.0026826266392971677\n",
      "Gradient Descent(136/499): loss=0.4106586196516203, w0=-0.2259533959167528, w1=-0.0022978706308556304\n",
      "Gradient Descent(137/499): loss=0.41041790296370023, w0=-0.22684346195759483, w1=-0.0019144811125385958\n",
      "Gradient Descent(138/499): loss=0.41018039648374405, w0=-0.22772462733802848, w1=-0.0015324457707550161\n",
      "Gradient Descent(139/499): loss=0.4099460393155021, w0=-0.2285969810646578, w1=-0.0011517525116380859\n",
      "Gradient Descent(140/499): loss=0.4097147719636286, w0=-0.22946061125402084, w1=-0.0007723894544668054\n",
      "Gradient Descent(141/499): loss=0.4094865362967289, w0=-0.23031560514149027, w1=-0.0003943449253506544\n",
      "Gradient Descent(142/499): loss=0.40926127551150376, w0=-0.231162049090085, w1=-1.760745116522963e-05\n",
      "Gradient Descent(143/499): loss=0.4090389340979509, w0=-0.23200002859919383, w1=0.0003578342462726584\n",
      "Gradient Descent(144/499): loss=0.4088194578055948, w0=-0.23282962831321158, w1=0.0007319912558014054\n",
      "Gradient Descent(145/499): loss=0.4086027936107048, w0=-0.23365093203008916, w1=0.0011048744822929704\n",
      "Gradient Descent(146/499): loss=0.4083888896844749, w0=-0.234464022709798, w1=0.0014764946518231303\n",
      "Gradient Descent(147/499): loss=0.40817769536213033, w0=-0.23526898248270975, w1=0.001846862316642996\n",
      "Gradient Descent(148/499): loss=0.4079691611129319, w0=-0.2360658926578924, w1=0.002215987859960787\n",
      "Gradient Descent(149/499): loss=0.40776323851105023, w0=-0.23685483373132327, w1=0.00258388150054243\n",
      "Gradient Descent(150/499): loss=0.407559880207281, w0=-0.23763588539401984, w1=0.0029505532971390454\n",
      "Gradient Descent(151/499): loss=0.4073590399015724, w0=-0.23840912654008944, w1=0.003316013152749087\n",
      "Gradient Descent(152/499): loss=0.4071606723163439, w0=-0.2391746352746984, w1=0.0036802708187224863\n",
      "Gradient Descent(153/499): loss=0.4069647331705648, w0=-0.23993248892196128, w1=0.004043335898713757\n",
      "Gradient Descent(154/499): loss=0.40677117915457367, w0=-0.24068276403275152, w1=0.004405217852490754\n",
      "Gradient Descent(155/499): loss=0.4065799679056114, w0=-0.2414255363924339, w1=0.004765925999605401\n",
      "Gradient Descent(156/499): loss=0.4063910579840481, w0=-0.24216088102851943, w1=0.005125469522932468\n",
      "Gradient Descent(157/499): loss=0.40620440885028086, w0=-0.24288887221824415, w1=0.005483857472082063\n",
      "Gradient Descent(158/499): loss=0.40601998084228086, w0=-0.24360958349607162, w1=0.005841098766691355\n",
      "Gradient Descent(159/499): loss=0.4058377351537693, w0=-0.24432308766112085, w1=0.006197202199600778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(160/499): loss=0.40565763381300557, w0=-0.2450294567845196, w1=0.0065521764399195425\n",
      "Gradient Descent(161/499): loss=0.40547963966216405, w0=-0.24572876221668438, w1=0.006906030035985363\n",
      "Gradient Descent(162/499): loss=0.4053037163372843, w0=-0.24642107459452753, w1=0.0072587714182227175\n",
      "Gradient Descent(163/499): loss=0.40512982824877625, w0=-0.24710646384859225, w1=0.007610408901904087\n",
      "Gradient Descent(164/499): loss=0.4049579405624642, w0=-0.24778499921011632, w1=0.007960950689818126\n",
      "Gradient Descent(165/499): loss=0.4047880191811501, w0=-0.24845674921802519, w1=0.008310404874848729\n",
      "Gradient Descent(166/499): loss=0.4046200307266832, w0=-0.24912178172585497, w1=0.00865877944246875\n",
      "Gradient Descent(167/499): loss=0.4044539425225213, w0=-0.24978016390860644, w1=0.009006082273151781\n",
      "Gradient Descent(168/499): loss=0.40428972257676377, w0=-0.2504319622695304, w1=0.009352321144705505\n",
      "Gradient Descent(169/499): loss=0.40412733956564834, w0=-0.25107724264684517, w1=0.009697503734529699\n",
      "Gradient Descent(170/499): loss=0.4039667628174934, w0=-0.2517160702203868, w1=0.01004163762180209\n",
      "Gradient Descent(171/499): loss=0.4038079622970723, w0=-0.25234850951819304, w1=0.01038473028959489\n",
      "Gradient Descent(172/499): loss=0.40365090859041, w0=-0.2529746244230212, w1=0.01072678912692484\n",
      "Gradient Descent(173/499): loss=0.4034955728899874, w0=-0.2535944781788011, w1=0.011067821430739434\n",
      "Gradient Descent(174/499): loss=0.4033419269803391, w0=-0.25420813339702325, w1=0.011407834407841827\n",
      "Gradient Descent(175/499): loss=0.4031899432240373, w0=-0.25481565206306317, w1=0.011746835176756879\n",
      "Gradient Descent(176/499): loss=0.40303959454804617, w0=-0.25541709554244274, w1=0.012084830769540669\n",
      "Gradient Descent(177/499): loss=0.402890854430439, w0=-0.2560125245870285, w1=0.012421828133535602\n",
      "Gradient Descent(178/499): loss=0.4027436968874655, w0=-0.2566019993411684, w1=0.01275783413307333\n",
      "Gradient Descent(179/499): loss=0.40259809646095984, w0=-0.2571855793477669, w1=0.013092855551127424\n",
      "Gradient Descent(180/499): loss=0.40245402820607834, w0=-0.2577633235542995, w1=0.01342689909091776\n",
      "Gradient Descent(181/499): loss=0.402311467679359, w0=-0.25833529031876673, w1=0.013759971377468396\n",
      "Gradient Descent(182/499): loss=0.40217039092709156, w0=-0.2589015374155893, w1=0.014092078959120808\n",
      "Gradient Descent(183/499): loss=0.4020307744739909, w0=-0.25946212204144364, w1=0.014423228309004039\n",
      "Gradient Descent(184/499): loss=0.4018925953121636, w0=-0.2600171008210395, w1=0.014753425826463445\n",
      "Gradient Descent(185/499): loss=0.4017558308903595, w0=-0.2605665298128394, w1=0.015082677838449536\n",
      "Gradient Descent(186/499): loss=0.4016204591035007, w0=-0.2611104645147213, w1=0.015410990600868422\n",
      "Gradient Descent(187/499): loss=0.40148645828248003, w0=-0.2616489598695844, w1=0.015738370299895176\n",
      "Gradient Descent(188/499): loss=0.4013538071842193, w0=-0.2621820702708989, w1=0.016064823053251555\n",
      "Gradient Descent(189/499): loss=0.40122248498198326, w0=-0.26270984956820026, w1=0.016390354911449325\n",
      "Gradient Descent(190/499): loss=0.4010924712559401, w0=-0.26323235107252857, w1=0.01671497185900037\n",
      "Gradient Descent(191/499): loss=0.40096374598395984, w0=-0.26374962756181364, w1=0.01703867981559485\n",
      "Gradient Descent(192/499): loss=0.4008362895326484, w0=-0.2642617312862059, w1=0.01736148463724844\n",
      "Gradient Descent(193/499): loss=0.4007100826486053, w0=-0.2647687139733542, w1=0.017683392117419833\n",
      "Gradient Descent(194/499): loss=0.40058510644990486, w0=-0.26527062683363106, w1=0.018004407988099415\n",
      "Gradient Descent(195/499): loss=0.40046134241778725, w0=-0.26576752056530517, w1=0.018324537920870198\n",
      "Gradient Descent(196/499): loss=0.4003387723885609, w0=-0.26625944535966256, w1=0.01864378752794193\n",
      "Gradient Descent(197/499): loss=0.40021737854570527, w0=-0.26674645090607635, w1=0.018962162363159278\n",
      "Gradient Descent(198/499): loss=0.40009714341217123, w0=-0.267228586397026, w1=0.019279667922984932\n",
      "Gradient Descent(199/499): loss=0.39997804984287055, w0=-0.2677059005330662, w1=0.01959630964745854\n",
      "Gradient Descent(200/499): loss=0.39986008101735454, w0=-0.268178441527746, w1=0.019912092921132194\n",
      "Gradient Descent(201/499): loss=0.3997432204326707, w0=-0.268646257112479, w1=0.02022702307398329\n",
      "Gradient Descent(202/499): loss=0.39962745189639604, w0=-0.26910939454136473, w1=0.02054110538230544\n",
      "Gradient Descent(203/499): loss=0.39951275951984205, w0=-0.2695679005959616, w1=0.020854345069578232\n",
      "Gradient Descent(204/499): loss=0.39939912771142494, w0=-0.2700218215900125, w1=0.0211667473073164\n",
      "Gradient Descent(205/499): loss=0.3992865411701985, w0=-0.2704712033741229, w1=0.021478317215899177\n",
      "Gradient Descent(206/499): loss=0.39917498487954434, w0=-0.2709160913403922, w1=0.021789059865380357\n",
      "Gradient Descent(207/499): loss=0.39906444410101494, w0=-0.2713565304269988, w1=0.022098980276279725\n",
      "Gradient Descent(208/499): loss=0.39895490436832626, w0=-0.27179256512273936, w1=0.02240808342035642\n",
      "Gradient Descent(209/499): loss=0.39884635148149505, w0=-0.2722242394715225, w1=0.022716374221364764\n",
      "Gradient Descent(210/499): loss=0.3987387715011176, w0=-0.27265159707681785, w1=0.023023857555793125\n",
      "Gradient Descent(211/499): loss=0.39863215074278646, w0=-0.27307468110606026, w1=0.023330538253586296\n",
      "Gradient Descent(212/499): loss=0.3985264757716392, w0=-0.2734935342950102, w1=0.02363642109885192\n",
      "Gradient Descent(213/499): loss=0.39842173339703973, w0=-0.2739081989520707, w1=0.023941510830551384\n",
      "Gradient Descent(214/499): loss=0.3983179106673844, w0=-0.2743187169625606, w1=0.024245812143175678\n",
      "Gradient Descent(215/499): loss=0.39821499486503376, w0=-0.2747251297929456, w1=0.024549329687406682\n",
      "Gradient Descent(216/499): loss=0.3981129735013611, w0=-0.2751274784950268, w1=0.02485206807076422\n",
      "Gradient Descent(217/499): loss=0.39801183431192266, w0=-0.27552580371008717, w1=0.02515403185823937\n",
      "Gradient Descent(218/499): loss=0.39791156525173704, w0=-0.27592014567299694, w1=0.0254552255729144\n",
      "Gradient Descent(219/499): loss=0.39781215449067975, w0=-0.2763105442162776, w1=0.02575565369656968\n",
      "Gradient Descent(220/499): loss=0.3977135904089833, w0=-0.2766970387741255, w1=0.02605532067027798\n",
      "Gradient Descent(221/499): loss=0.3976158615928429, w0=-0.2770796683863949, w1=0.02635423089498647\n",
      "Gradient Descent(222/499): loss=0.39751895683012634, w0=-0.2774584717025416, w1=0.026652388732086772\n",
      "Gradient Descent(223/499): loss=0.39742286510618186, w0=-0.2778334869855269, w1=0.026949798503973416\n",
      "Gradient Descent(224/499): loss=0.3973275755997448, w0=-0.27820475211568235, w1=0.027246464494590967\n",
      "Gradient Descent(225/499): loss=0.39723307767893945, w0=-0.27857230459453625, w1=0.027542390949970165\n",
      "Gradient Descent(226/499): loss=0.3971393608973706, w0=-0.2789361815486016, w1=0.02783758207875337\n",
      "Gradient Descent(227/499): loss=0.3970464149903092, w0=-0.2792964197331263, w1=0.028132042052709593\n",
      "Gradient Descent(228/499): loss=0.39695422987096235, w0=-0.2796530555358058, w1=0.02842577500723935\n",
      "Gradient Descent(229/499): loss=0.3968627956268321, w0=-0.28000612498045846, w1=0.028718785041869664\n",
      "Gradient Descent(230/499): loss=0.39677210251615425, w0=-0.2803556637306646, w1=0.029011076220739476\n",
      "Gradient Descent(231/499): loss=0.3966821409644208, w0=-0.2807017070933687, w1=0.029302652573075596\n",
      "Gradient Descent(232/499): loss=0.3965929015609811, w0=-0.2810442900224458, w1=0.029593518093659607\n",
      "Gradient Descent(233/499): loss=0.3965043750557211, w0=-0.2813834471222321, w1=0.029883676743285806\n",
      "Gradient Descent(234/499): loss=0.396416552355815, w0=-0.28171921265102057, w1=0.030173132449210494\n",
      "Gradient Descent(235/499): loss=0.3963294245225547, w0=-0.28205162052452115, w1=0.0304618891055928\n",
      "Gradient Descent(236/499): loss=0.39624298276824726, w0=-0.28238070431928675, w1=0.030749950573927244\n",
      "Gradient Descent(237/499): loss=0.39615721845318425, w0=-0.2827064972761047, w1=0.031037320683468254\n",
      "Gradient Descent(238/499): loss=0.3960721230826781, w0=-0.28302903230335447, w1=0.03132400323164684\n",
      "Gradient Descent(239/499): loss=0.3959876883041654, w0=-0.28334834198033176, w1=0.0316100019844796\n",
      "Gradient Descent(240/499): loss=0.3959039059043758, w0=-0.2836644585605393, w1=0.03189532067697024\n",
      "Gradient Descent(241/499): loss=0.39582076780656245, w0=-0.28397741397494475, w1=0.032179963013503815\n",
      "Gradient Descent(242/499): loss=0.39573826606779505, w0=-0.28428723983520615, w1=0.03246393266823381\n",
      "Gradient Descent(243/499): loss=0.395656392876313, w0=-0.28459396743686494, w1=0.032747233285462314\n",
      "Gradient Descent(244/499): loss=0.3955751405489376, w0=-0.28489762776250716, w1=0.03302986848001338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(245/499): loss=0.395494501528541, w0=-0.285198251484893, w1=0.033311841837599714\n",
      "Gradient Descent(246/499): loss=0.3954144683815704, w0=-0.2854958689700549, w1=0.03359315691518293\n",
      "Gradient Descent(247/499): loss=0.39533503379562807, w0=-0.28579051028036523, w1=0.03387381724132747\n",
      "Gradient Descent(248/499): loss=0.3952561905771039, w0=-0.2860822051775725, w1=0.03415382631654833\n",
      "Gradient Descent(249/499): loss=0.3951779316488599, w0=-0.28637098312580767, w1=0.034433187613652674\n",
      "Gradient Descent(250/499): loss=0.3951002500479654, w0=-0.2866568732945605, w1=0.03471190457807567\n",
      "Gradient Descent(251/499): loss=0.3950231389234812, w0=-0.2869399045616258, w1=0.03498998062821043\n",
      "Gradient Descent(252/499): loss=0.394946591534293, w0=-0.2872201055160205, w1=0.03526741915573234\n",
      "Gradient Descent(253/499): loss=0.39487060124699164, w0=-0.28749750446087124, w1=0.035544223525917885\n",
      "Gradient Descent(254/499): loss=0.39479516153379807, w0=-0.2877721294162735, w1=0.03582039707795806\n",
      "Gradient Descent(255/499): loss=0.39472026597053483, w0=-0.2880440081221217, w1=0.03609594312526647\n",
      "Gradient Descent(256/499): loss=0.3946459082346411, w0=-0.28831316804091145, w1=0.03637086495578228\n",
      "Gradient Descent(257/499): loss=0.3945720821032292, w0=-0.2885796363605133, w1=0.03664516583226815\n",
      "Gradient Descent(258/499): loss=0.39449878145118455, w0=-0.28884343999691914, w1=0.036918848992603115\n",
      "Gradient Descent(259/499): loss=0.39442600024930563, w0=-0.28910460559696094, w1=0.03719191765007074\n",
      "Gradient Descent(260/499): loss=0.3943537325624837, w0=-0.28936315954100233, w1=0.03746437499364244\n",
      "Gradient Descent(261/499): loss=0.3942819725479213, w0=-0.2896191279456033, w1=0.03773622418825623\n",
      "Gradient Descent(262/499): loss=0.39421071445339, w0=-0.28987253666615825, w1=0.038007468375090885\n",
      "Gradient Descent(263/499): loss=0.39413995261552265, w0=-0.2901234112995077, w1=0.038278110671835675\n",
      "Gradient Descent(264/499): loss=0.3940696814581451, w0=-0.2903717771865236, w1=0.03854815417295573\n",
      "Gradient Descent(265/499): loss=0.39399989549064063, w0=-0.29061765941466944, w1=0.03881760194995316\n",
      "Gradient Descent(266/499): loss=0.39393058930635044, w0=-0.29086108282053375, w1=0.03908645705162399\n",
      "Gradient Descent(267/499): loss=0.3938617575810073, w0=-0.29110207199233945, w1=0.03935472250431099\n",
      "Gradient Descent(268/499): loss=0.39379339507120237, w0=-0.2913406512724271, w1=0.03962240131215252\n",
      "Gradient Descent(269/499): loss=0.393725496612885, w0=-0.29157684475971385, w1=0.03988949645732741\n",
      "Gradient Descent(270/499): loss=0.3936580571198914, w0=-0.29181067631212776, w1=0.04015601090029605\n",
      "Gradient Descent(271/499): loss=0.3935910715825084, w0=-0.2920421695490175, w1=0.040421947580037663\n",
      "Gradient Descent(272/499): loss=0.3935245350660626, w0=-0.2922713478535384, w1=0.0406873094142839\n",
      "Gradient Descent(273/499): loss=0.3934584427095427, w0=-0.2924982343750141, w1=0.0409520992997488\n",
      "Gradient Descent(274/499): loss=0.393392789724248, w0=-0.29272285203127507, w1=0.041216320112355244\n",
      "Gradient Descent(275/499): loss=0.3933275713924662, w0=-0.2929452235109734, w1=0.041479974707457866\n",
      "Gradient Descent(276/499): loss=0.39326278306617884, w0=-0.2931653712758748, w1=0.04174306592006259\n",
      "Gradient Descent(277/499): loss=0.3931984201657918, w0=-0.29338331756312713, w1=0.04200559656504283\n",
      "Gradient Descent(278/499): loss=0.3931344781788941, w0=-0.293599084387507, w1=0.04226756943735239\n",
      "Gradient Descent(279/499): loss=0.39307095265904135, w0=-0.29381269354364303, w1=0.04252898731223515\n",
      "Gradient Descent(280/499): loss=0.3930078392245639, w0=-0.29402416660821773, w1=0.04278985294543162\n",
      "Gradient Descent(281/499): loss=0.3929451335573999, w0=-0.29423352494214666, w1=0.043050169073382334\n",
      "Gradient Descent(282/499): loss=0.39288283140195235, w0=-0.29444078969273635, w1=0.04330993841342833\n",
      "Gradient Descent(283/499): loss=0.39282092856396883, w0=-0.29464598179582013, w1=0.043569163664008544\n",
      "Gradient Descent(284/499): loss=0.3927594209094453, w0=-0.2948491219778731, w1=0.0438278475048543\n",
      "Gradient Descent(285/499): loss=0.39269830436355063, w0=-0.29505023075810555, w1=0.044085992597181003\n",
      "Gradient Descent(286/499): loss=0.39263757490957485, w0=-0.29524932845053564, w1=0.04434360158387695\n",
      "Gradient Descent(287/499): loss=0.39257722858789684, w0=-0.29544643516604147, w1=0.04460067708968941\n",
      "Gradient Descent(288/499): loss=0.39251726149497407, w0=-0.29564157081439224, w1=0.04485722172140799\n",
      "Gradient Descent(289/499): loss=0.39245766978235297, w0=-0.2958347551062595, w1=0.04511323806804537\n",
      "Gradient Descent(290/499): loss=0.3923984496556985, w0=-0.2960260075552081, w1=0.045368728701015375\n",
      "Gradient Descent(291/499): loss=0.3923395973738426, w0=-0.2962153474796672, w1=0.045623696174308545\n",
      "Gradient Descent(292/499): loss=0.3922811092478547, w0=-0.29640279400488173, w1=0.045878143024665176\n",
      "Gradient Descent(293/499): loss=0.39222298164012637, w0=-0.2965883660648441, w1=0.046132071771745865\n",
      "Gradient Descent(294/499): loss=0.39216521096347945, w0=-0.2967720824042069, w1=0.04638548491829968\n",
      "Gradient Descent(295/499): loss=0.3921077936802867, w0=-0.296953961580176, w1=0.0466383849503299\n",
      "Gradient Descent(296/499): loss=0.39205072630161464, w0=-0.2971340219643855, w1=0.04689077433725749\n",
      "Gradient Descent(297/499): loss=0.39199400538638024, w0=-0.2973122817447529, w1=0.0471426555320822\n",
      "Gradient Descent(298/499): loss=0.3919376275405258, w0=-0.2974887589273166, w1=0.04739403097154151\n",
      "Gradient Descent(299/499): loss=0.39188158941620954, w0=-0.29766347133805465, w1=0.04764490307626725\n",
      "Gradient Descent(300/499): loss=0.39182588771101357, w0=-0.29783643662468534, w1=0.04789527425094015\n",
      "Gradient Descent(301/499): loss=0.39177051916716527, w0=-0.29800767225844976, w1=0.048145146884442214\n",
      "Gradient Descent(302/499): loss=0.3917154805707758, w0=-0.2981771955358765, w1=0.04839452335000699\n",
      "Gradient Descent(303/499): loss=0.3916607687510929, w0=-0.298345023580529, w1=0.04864340600536779\n",
      "Gradient Descent(304/499): loss=0.39160638057976854, w0=-0.298511173344735, w1=0.04889179719290393\n",
      "Gradient Descent(305/499): loss=0.39155231297014026, w0=-0.2986756616112989, w1=0.04913969923978489\n",
      "Gradient Descent(306/499): loss=0.3914985628765282, w0=-0.29883850499519715, w1=0.04938711445811261\n",
      "Gradient Descent(307/499): loss=0.39144512729354375, w0=-0.2989997199452565, w1=0.04963404514506186\n",
      "Gradient Descent(308/499): loss=0.39139200325541285, w0=-0.2991593227458152, w1=0.04988049358301866\n",
      "Gradient Descent(309/499): loss=0.3913391878353127, w0=-0.2993173295183683, w1=0.05012646203971692\n",
      "Gradient Descent(310/499): loss=0.39128667814472073, w0=-0.29947375622319594, w1=0.05037195276837322\n",
      "Gradient Descent(311/499): loss=0.3912344713327762, w0=-0.2996286186609753, w1=0.05061696800781981\n",
      "Gradient Descent(312/499): loss=0.39118256458565426, w0=-0.29978193247437684, w1=0.050861509982635895\n",
      "Gradient Descent(313/499): loss=0.39113095512595225, w0=-0.2999337131496444, w1=0.05110558090327709\n",
      "Gradient Descent(314/499): loss=0.3910796402120879, w0=-0.3000839760181593, w1=0.05134918296620329\n",
      "Gradient Descent(315/499): loss=0.39102861713770837, w0=-0.300232736257989, w1=0.05159231835400481\n",
      "Gradient Descent(316/499): loss=0.39097788323111127, w0=-0.30038000889542044, w1=0.05183498923552688\n",
      "Gradient Descent(317/499): loss=0.39092743585467715, w0=-0.30052580880647756, w1=0.05207719776599257\n",
      "Gradient Descent(318/499): loss=0.3908772724043115, w0=-0.3006701507184241, w1=0.052318946087124056\n",
      "Gradient Descent(319/499): loss=0.3908273903088989, w0=-0.30081304921125124, w1=0.05256023632726243\n",
      "Gradient Descent(320/499): loss=0.39077778702976584, w0=-0.3009545187191501, w1=0.05280107060148587\n",
      "Gradient Descent(321/499): loss=0.3907284600601558, w0=-0.30109457353196994, w1=0.053041451011726366\n",
      "Gradient Descent(322/499): loss=0.3906794069247132, w0=-0.3012332277966616, w1=0.053281379646884945\n",
      "Gradient Descent(323/499): loss=0.3906306251789756, w0=-0.30137049551870637, w1=0.05352085858294543\n",
      "Gradient Descent(324/499): loss=0.3905821124088797, w0=-0.3015063905635307, w1=0.05375988988308679\n",
      "Gradient Descent(325/499): loss=0.39053386623027064, w0=-0.30164092665790676, w1=0.05399847559779405\n",
      "Gradient Descent(326/499): loss=0.3904858842884271, w0=-0.3017741173913391, w1=0.05423661776496782\n",
      "Gradient Descent(327/499): loss=0.3904381642575896, w0=-0.3019059762174371, w1=0.05447431841003249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(328/499): loss=0.3903907038405017, w0=-0.30203651645527413, w1=0.054711579546043\n",
      "Gradient Descent(329/499): loss=0.39034350076795715, w0=-0.30216575129073275, w1=0.054948403173790426\n",
      "Gradient Descent(330/499): loss=0.39029655279835684, w0=-0.3022936937778368, w1=0.05518479128190612\n",
      "Gradient Descent(331/499): loss=0.39024985771727455, w0=-0.30242035684006985, w1=0.055420745846964665\n",
      "Gradient Descent(332/499): loss=0.3902034133370279, w0=-0.30254575327168054, w1=0.05565626883358553\n",
      "Gradient Descent(333/499): loss=0.39015721749626137, w0=-0.3026698957389751, w1=0.055891362194533514\n",
      "Gradient Descent(334/499): loss=0.39011126805953344, w0=-0.30279279678159676, w1=0.05612602787081797\n",
      "Gradient Descent(335/499): loss=0.39006556291691374, w0=-0.3029144688137922, w1=0.05636026779179077\n",
      "Gradient Descent(336/499): loss=0.3900200999835864, w0=-0.3030349241256657, w1=0.05659408387524321\n",
      "Gradient Descent(337/499): loss=0.38997487719946045, w0=-0.3031541748844205, w1=0.056827478027501634\n",
      "Gradient Descent(338/499): loss=0.3899298925287892, w0=-0.3032722331355877, w1=0.05706045214352197\n",
      "Gradient Descent(339/499): loss=0.38988514395979407, w0=-0.30338911080424324, w1=0.057293008106983165\n",
      "Gradient Descent(340/499): loss=0.3898406295042973, w0=-0.30350481969621224, w1=0.057525147790379455\n",
      "Gradient Descent(341/499): loss=0.3897963471973605, w0=-0.30361937149926155, w1=0.05775687305511159\n",
      "Gradient Descent(342/499): loss=0.3897522950969299, w0=-0.30373277778428037, w1=0.057988185751576986\n",
      "Gradient Descent(343/499): loss=0.3897084712834878, w0=-0.303845050006449, w1=0.058219087719258766\n",
      "Gradient Descent(344/499): loss=0.38966487385971077, w0=-0.30395619950639596, w1=0.058449580786813865\n",
      "Gradient Descent(345/499): loss=0.3896215009501346, w0=-0.3040662375113435, w1=0.05867966677215998\n",
      "Gradient Descent(346/499): loss=0.3895783507008227, w0=-0.30417517513624154, w1=0.058909347482561644\n",
      "Gradient Descent(347/499): loss=0.38953542127904456, w0=-0.3042830233848906, w1=0.059138624714715204\n",
      "Gradient Descent(348/499): loss=0.38949271087295645, w0=-0.3043897931510532, w1=0.059367500254832895\n",
      "Gradient Descent(349/499): loss=0.38945021769128907, w0=-0.30449549521955416, w1=0.0595959758787259\n",
      "Gradient Descent(350/499): loss=0.38940793996304185, w0=-0.3046001402673701, w1=0.0598240533518865\n",
      "Gradient Descent(351/499): loss=0.38936587593718125, w0=-0.30470373886470786, w1=0.06005173442956927\n",
      "Gradient Descent(352/499): loss=0.3893240238823446, w0=-0.3048063014760723, w1=0.06027902085687136\n",
      "Gradient Descent(353/499): loss=0.3892823820865503, w0=-0.3049078384613231, w1=0.06050591436881188\n",
      "Gradient Descent(354/499): loss=0.3892409488569116, w0=-0.30500836007672133, w1=0.060732416690410365\n",
      "Gradient Descent(355/499): loss=0.3891997225193576, w0=-0.3051078764759656, w1=0.060958529536764436\n",
      "Gradient Descent(356/499): loss=0.38915870141835623, w0=-0.30520639771121744, w1=0.06118425461312649\n",
      "Gradient Descent(357/499): loss=0.3891178839166452, w0=-0.3053039337341168, w1=0.06140959361497963\n",
      "Gradient Descent(358/499): loss=0.3890772683949658, w0=-0.3054004943967871, w1=0.0616345482281127\n",
      "Gradient Descent(359/499): loss=0.3890368532518012, w0=-0.3054960894528308, w1=0.06185912012869452\n",
      "Gradient Descent(360/499): loss=0.3889966369031208, w0=-0.30559072855831404, w1=0.06208331098334729\n",
      "Gradient Descent(361/499): loss=0.38895661778212826, w0=-0.3056844212727424, w1=0.06230712244921924\n",
      "Gradient Descent(362/499): loss=0.38891679433901316, w0=-0.30577717706002655, w1=0.06253055617405638\n",
      "Gradient Descent(363/499): loss=0.3888771650407085, w0=-0.30586900528943783, w1=0.06275361379627363\n",
      "Gradient Descent(364/499): loss=0.3888377283706512, w0=-0.30595991523655497, w1=0.06297629694502502\n",
      "Gradient Descent(365/499): loss=0.38879848282854756, w0=-0.30604991608420096, w1=0.06319860724027322\n",
      "Gradient Descent(366/499): loss=0.3887594269301421, w0=-0.3061390169233705, w1=0.06342054629285837\n",
      "Gradient Descent(367/499): loss=0.38872055920699067, w0=-0.3062272267541483, w1=0.06364211570456606\n",
      "Gradient Descent(368/499): loss=0.3886818782062382, w0=-0.3063145544866184, w1=0.06386331706819465\n",
      "Gradient Descent(369/499): loss=0.38864338249039865, w0=-0.30640100894176375, w1=0.06408415196762186\n",
      "Gradient Descent(370/499): loss=0.3886050706371404, w0=-0.3064865988523577, w1=0.06430462197787074\n",
      "Gradient Descent(371/499): loss=0.3885669412390743, w0=-0.3065713328638457, w1=0.06452472866517478\n",
      "Gradient Descent(372/499): loss=0.3885289929035454, w0=-0.3066552195352188, w1=0.06474447358704248\n",
      "Gradient Descent(373/499): loss=0.38849122425242943, w0=-0.3067382673398782, w1=0.06496385829232121\n",
      "Gradient Descent(374/499): loss=0.38845363392193033, w0=-0.306820484666491, w1=0.06518288432126033\n",
      "Gradient Descent(375/499): loss=0.3884162205623835, w0=-0.30690187981983763, w1=0.06540155320557378\n",
      "Gradient Descent(376/499): loss=0.3883789828380622, w0=-0.30698246102165083, w1=0.06561986646850194\n",
      "Gradient Descent(377/499): loss=0.38834191942698504, w0=-0.3070622364114459, w1=0.06583782562487288\n",
      "Gradient Descent(378/499): loss=0.3883050290207293, w0=-0.30714121404734307, w1=0.06605543218116294\n",
      "Gradient Descent(379/499): loss=0.3882683103242459, w0=-0.30721940190688124, w1=0.0662726876355568\n",
      "Gradient Descent(380/499): loss=0.3882317620556786, w0=-0.30729680788782404, w1=0.06648959347800681\n",
      "Gradient Descent(381/499): loss=0.3881953829461843, w0=-0.3073734398089574, w1=0.0667061511902918\n",
      "Gradient Descent(382/499): loss=0.38815917173975856, w0=-0.30744930541087945, w1=0.06692236224607527\n",
      "Gradient Descent(383/499): loss=0.3881231271930629, w0=-0.30752441235678224, w1=0.06713822811096298\n",
      "Gradient Descent(384/499): loss=0.3880872480752543, w0=-0.307598768233226, w1=0.06735375024256005\n",
      "Gradient Descent(385/499): loss=0.38805153316781993, w0=-0.3076723805509054, w1=0.06756893009052732\n",
      "Gradient Descent(386/499): loss=0.3880159812644111, w0=-0.307745256745408, w1=0.06778376909663733\n",
      "Gradient Descent(387/499): loss=0.3879805911706831, w0=-0.30781740417796555, w1=0.06799826869482961\n",
      "Gradient Descent(388/499): loss=0.3879453617041365, w0=-0.3078888301361975, w1=0.06821243031126553\n",
      "Gradient Descent(389/499): loss=0.3879102916939604, w0=-0.30795954183484714, w1=0.06842625536438254\n",
      "Gradient Descent(390/499): loss=0.3878753799808786, w0=-0.3080295464165103, w1=0.06863974526494787\n",
      "Gradient Descent(391/499): loss=0.3878406254169998, w0=-0.30809885095235684, w1=0.06885290141611179\n",
      "Gradient Descent(392/499): loss=0.3878060268656677, w0=-0.30816746244284493, w1=0.06906572521346024\n",
      "Gradient Descent(393/499): loss=0.38777158320131566, w0=-0.30823538781842813, w1=0.06927821804506706\n",
      "Gradient Descent(394/499): loss=0.3877372933093219, w0=-0.3083026339402555, w1=0.06949038129154561\n",
      "Gradient Descent(395/499): loss=0.38770315608586936, w0=-0.30836920760086456, w1=0.06970221632609996\n",
      "Gradient Descent(396/499): loss=0.387669170437805, w0=-0.3084351155248676, w1=0.06991372451457563\n",
      "Gradient Descent(397/499): loss=0.3876353352825044, w0=-0.3085003643696306, w1=0.0701249072155097\n",
      "Gradient Descent(398/499): loss=0.38760164954773524, w0=-0.3085649607259459, w1=0.07033576578018057\n",
      "Gradient Descent(399/499): loss=0.3875681121715263, w0=-0.3086289111186981, w1=0.07054630155265726\n",
      "Gradient Descent(400/499): loss=0.38753472210203616, w0=-0.3086922220075228, w1=0.07075651586984812\n",
      "Gradient Descent(401/499): loss=0.3875014782974246, w0=-0.30875489978745924, w1=0.07096641006154916\n",
      "Gradient Descent(402/499): loss=0.3874683797257269, w0=-0.3088169507895963, w1=0.071175985450492\n",
      "Gradient Descent(403/499): loss=0.3874354253647291, w0=-0.308878381281712, w1=0.07138524335239121\n",
      "Gradient Descent(404/499): loss=0.3874026142018455, w0=-0.3089391974689066, w1=0.07159418507599134\n",
      "Gradient Descent(405/499): loss=0.38736994523399815, w0=-0.3089994054942292, w1=0.07180281192311343\n",
      "Gradient Descent(406/499): loss=0.38733741746749895, w0=-0.3090590114392986, w1=0.07201112518870116\n",
      "Gradient Descent(407/499): loss=0.3873050299179325, w0=-0.3091180213249173, w1=0.07221912616086654\n",
      "Gradient Descent(408/499): loss=0.38727278161004075, w0=-0.3091764411116798, w1=0.07242681612093516\n",
      "Gradient Descent(409/499): loss=0.3872406715776106, w0=-0.3092342767005747, w1=0.07263419634349105\n",
      "Gradient Descent(410/499): loss=0.3872086988633628, w0=-0.30929153393358066, w1=0.07284126809642116\n",
      "Gradient Descent(411/499): loss=0.3871768625188413, w0=-0.30934821859425654, w1=0.07304803264095935\n",
      "Gradient Descent(412/499): loss=0.3871451616043064, w0=-0.3094043364083257, w1=0.07325449123173008\n",
      "Gradient Descent(413/499): loss=0.38711359518862837, w0=-0.30945989304425414, w1=0.07346064511679165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(414/499): loss=0.387082162349182, w0=-0.3095148941138233, w1=0.07366649553767904\n",
      "Gradient Descent(415/499): loss=0.387050862171745, w0=-0.3095693451726968, w1=0.07387204372944642\n",
      "Gradient Descent(416/499): loss=0.3870196937503957, w0=-0.30962325172098154, w1=0.07407729092070924\n",
      "Gradient Descent(417/499): loss=0.38698865618741324, w0=-0.3096766192037834, w1=0.07428223833368594\n",
      "Gradient Descent(418/499): loss=0.3869577485931797, w0=-0.3097294530117573, w1=0.07448688718423932\n",
      "Gradient Descent(419/499): loss=0.3869269700860833, w0=-0.30978175848165146, w1=0.07469123868191752\n",
      "Gradient Descent(420/499): loss=0.38689631979242234, w0=-0.30983354089684667, w1=0.07489529402999466\n",
      "Gradient Descent(421/499): loss=0.3868657968463117, w0=-0.30988480548788994, w1=0.07509905442551108\n",
      "Gradient Descent(422/499): loss=0.38683540038959136, w0=-0.30993555743302276, w1=0.0753025210593133\n",
      "Gradient Descent(423/499): loss=0.3868051295717334, w0=-0.3099858018587043, w1=0.07550569511609356\n",
      "Gradient Descent(424/499): loss=0.3867749835497538, w0=-0.310035543840129, w1=0.07570857777442908\n",
      "Gradient Descent(425/499): loss=0.38674496148812343, w0=-0.3100847884017395, w1=0.07591117020682087\n",
      "Gradient Descent(426/499): loss=0.3867150625586812, w0=-0.31013354051773384, w1=0.07611347357973239\n",
      "Gradient Descent(427/499): loss=0.3866852859405482, w0=-0.31018180511256827, w1=0.07631548905362769\n",
      "Gradient Descent(428/499): loss=0.38665563082004345, w0=-0.3102295870614543, w1=0.07651721778300932\n",
      "Gradient Descent(429/499): loss=0.3866260963906001, w0=-0.31027689119085156, w1=0.07671866091645593\n",
      "Gradient Descent(430/499): loss=0.3865966818526842, w0=-0.3103237222789548, w1=0.07691981959665949\n",
      "Gradient Descent(431/499): loss=0.3865673864137135, w0=-0.310370085056177, w1=0.07712069496046217\n",
      "Gradient Descent(432/499): loss=0.3865382092879776, w0=-0.310415984205627, w1=0.0773212881388931\n",
      "Gradient Descent(433/499): loss=0.38650914969656003, w0=-0.3104614243635825, w1=0.07752160025720452\n",
      "Gradient Descent(434/499): loss=0.3864802068672609, w0=-0.31050641011995844, w1=0.0777216324349079\n",
      "Gradient Descent(435/499): loss=0.38645138003452034, w0=-0.31055094601877065, w1=0.07792138578580961\n",
      "Gradient Descent(436/499): loss=0.3864226684393438, w0=-0.3105950365585947, w1=0.0781208614180463\n",
      "Gradient Descent(437/499): loss=0.38639407132922793, w0=-0.31063868619302054, w1=0.07832006043412006\n",
      "Gradient Descent(438/499): loss=0.38636558795808773, w0=-0.31068189933110213, w1=0.07851898393093315\n",
      "Gradient Descent(439/499): loss=0.38633721758618483, w0=-0.31072468033780287, w1=0.07871763299982265\n",
      "Gradient Descent(440/499): loss=0.38630895948005667, w0=-0.31076703353443663, w1=0.07891600872659464\n",
      "Gradient Descent(441/499): loss=0.38628081291244637, w0=-0.31080896319910406, w1=0.07911411219155816\n",
      "Gradient Descent(442/499): loss=0.3862527771622342, w0=-0.3108504735671248, w1=0.07931194446955891\n",
      "Gradient Descent(443/499): loss=0.3862248515143698, w0=-0.31089156883146535, w1=0.07950950663001269\n",
      "Gradient Descent(444/499): loss=0.3861970352598056, w0=-0.3109322531431625, w1=0.07970679973693846\n",
      "Gradient Descent(445/499): loss=0.38616932769542983, w0=-0.3109725306117426, w1=0.07990382484899133\n",
      "Gradient Descent(446/499): loss=0.386141728124003, w0=-0.311012405305637, w1=0.08010058301949509\n",
      "Gradient Descent(447/499): loss=0.386114235854093, w0=-0.3110518812525924, w1=0.08029707529647452\n",
      "Gradient Descent(448/499): loss=0.3860868502000116, w0=-0.31109096244007833, w1=0.08049330272268757\n",
      "Gradient Descent(449/499): loss=0.3860595704817535, w0=-0.3111296528156894, w1=0.08068926633565708\n",
      "Gradient Descent(450/499): loss=0.3860323960249335, w0=-0.3111679562875443, w1=0.08088496716770247\n",
      "Gradient Descent(451/499): loss=0.3860053261607267, w0=-0.31120587672468064, w1=0.08108040624597093\n",
      "Gradient Descent(452/499): loss=0.38597836022580867, w0=-0.31124341795744564, w1=0.08127558459246859\n",
      "Gradient Descent(453/499): loss=0.3859514975622965, w0=-0.31128058377788304, w1=0.08147050322409129\n",
      "Gradient Descent(454/499): loss=0.38592473751769124, w0=-0.31131737794011605, w1=0.0816651631526552\n",
      "Gradient Descent(455/499): loss=0.38589807944481946, w0=-0.3113538041607267, w1=0.08185956538492715\n",
      "Gradient Descent(456/499): loss=0.38587152270177794, w0=-0.3113898661191313, w1=0.08205371092265472\n",
      "Gradient Descent(457/499): loss=0.38584506665187757, w0=-0.3114255674579518, w1=0.08224760076259609\n",
      "Gradient Descent(458/499): loss=0.38581871066358836, w0=-0.31146091178338414, w1=0.08244123589654972\n",
      "Gradient Descent(459/499): loss=0.38579245411048463, w0=-0.31149590266556215, w1=0.08263461731138376\n",
      "Gradient Descent(460/499): loss=0.3857662963711935, w0=-0.31153054363891836, w1=0.08282774598906516\n",
      "Gradient Descent(461/499): loss=0.38574023682933917, w0=-0.311564838202541, w1=0.08302062290668868\n",
      "Gradient Descent(462/499): loss=0.385714274873494, w0=-0.3115987898205274, w1=0.08321324903650563\n",
      "Gradient Descent(463/499): loss=0.385688409897125, w0=-0.311632401922334, w1=0.08340562534595232\n",
      "Gradient Descent(464/499): loss=0.38566264129854416, w0=-0.3116656779031225, w1=0.08359775279767837\n",
      "Gradient Descent(465/499): loss=0.3856369684808581, w0=-0.3116986211241031, w1=0.08378963234957482\n",
      "Gradient Descent(466/499): loss=0.385611390851919, w0=-0.31173123491287397, w1=0.08398126495480195\n",
      "Gradient Descent(467/499): loss=0.38558590782427593, w0=-0.3117635225637571, w1=0.08417265156181693\n",
      "Gradient Descent(468/499): loss=0.38556051881512693, w0=-0.3117954873381314, w1=0.08436379311440129\n",
      "Gradient Descent(469/499): loss=0.38553522324627143, w0=-0.31182713246476196, w1=0.08455469055168811\n",
      "Gradient Descent(470/499): loss=0.38551002054406364, w0=-0.3118584611401262, w1=0.0847453448081891\n",
      "Gradient Descent(471/499): loss=0.38548491013936687, w0=-0.31188947652873683, w1=0.08493575681382136\n",
      "Gradient Descent(472/499): loss=0.3854598914675065, w0=-0.3119201817634613, w1=0.08512592749393405\n",
      "Gradient Descent(473/499): loss=0.38543496396822724, w0=-0.3119505799458386, w1=0.08531585776933479\n",
      "Gradient Descent(474/499): loss=0.38541012708564726, w0=-0.3119806741463921, w1=0.08550554855631588\n",
      "Gradient Descent(475/499): loss=0.3853853802682148, w0=-0.31201046740494004, w1=0.08569500076668038\n",
      "Gradient Descent(476/499): loss=0.3853607229686651, w0=-0.3120399627309025, w1=0.0858842153077679\n",
      "Gradient Descent(477/499): loss=0.3853361546439776, w0=-0.3120691631036054, w1=0.08607319308248026\n",
      "Gradient Descent(478/499): loss=0.3853116747553341, w0=-0.31209807147258123, w1=0.086261934989307\n",
      "Gradient Descent(479/499): loss=0.3852872827680771, w0=-0.3121266907578673, w1=0.08645044192235055\n",
      "Gradient Descent(480/499): loss=0.3852629781516687, w0=-0.31215502385030053, w1=0.0866387147713514\n",
      "Gradient Descent(481/499): loss=0.38523876037965, w0=-0.3121830736118094, w1=0.08682675442171303\n",
      "Gradient Descent(482/499): loss=0.3852146289296012, w0=-0.3122108428757032, w1=0.08701456175452654\n",
      "Gradient Descent(483/499): loss=0.38519058328310274, w0=-0.3122383344469581, w1=0.08720213764659526\n",
      "Gradient Descent(484/499): loss=0.38516662292569515, w0=-0.3122655511025004, w1=0.08738948297045909\n",
      "Gradient Descent(485/499): loss=0.3851427473468415, w0=-0.3122924955914873, w1=0.08757659859441873\n",
      "Gradient Descent(486/499): loss=0.3851189560398889, w0=-0.31231917063558434, w1=0.08776348538255963\n",
      "Gradient Descent(487/499): loss=0.38509524850203175, w0=-0.3123455789292404, w1=0.08795014419477588\n",
      "Gradient Descent(488/499): loss=0.38507162423427355, w0=-0.3123717231399599, w1=0.08813657588679391\n",
      "Gradient Descent(489/499): loss=0.3850480827413909, w0=-0.31239760590857224, w1=0.08832278131019591\n",
      "Gradient Descent(490/499): loss=0.3850246235318974, w0=-0.31242322984949844, w1=0.08850876131244326\n",
      "Gradient Descent(491/499): loss=0.38500124611800834, w0=-0.3124485975510154, w1=0.0886945167368996\n",
      "Gradient Descent(492/499): loss=0.3849779500156042, w0=-0.3124737115755172, w1=0.08888004842285394\n",
      "Gradient Descent(493/499): loss=0.38495473474419717, w0=-0.3124985744597739, w1=0.08906535720554341\n",
      "Gradient Descent(494/499): loss=0.38493159982689606, w0=-0.3125231887151881, w1=0.08925044391617604\n",
      "Gradient Descent(495/499): loss=0.38490854479037256, w0=-0.31254755682804813, w1=0.08943530938195315\n",
      "Gradient Descent(496/499): loss=0.38488556916482747, w0=-0.3125716812597796, w1=0.08961995442609187\n",
      "Gradient Descent(497/499): loss=0.38486267248395767, w0=-0.3125955644471937, w1=0.08980437986784723\n",
      "Gradient Descent(498/499): loss=0.3848398542849236, w0=-0.3126192088027337, w1=0.08998858652253425\n",
      "Gradient Descent(499/499): loss=0.3848171141083166, w0=-0.31264261671471827, w1=0.09017257520154984\n",
      "Gradient Descent: execution time=29.594 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "import datetime\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialize weights\n",
    "w_initial = np.random.normal(0, 1e-1, X_train_poly.shape[1])\n",
    "print(w)\n",
    "print(w.shape)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y_train, X_train_poly, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([1 if x>0 else -1 for x in X_test_poly.dot(gradient_ws[-1].T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 71.2 %\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %.1f %%' % (np.mean(p == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([1 if x>0 else -1 for x in X_test_poly.dot(w.T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 77.6 %\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %.1f %%' % (np.mean(p == y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.delete(tX_test, ind_delete, axis=1)\n",
    "\n",
    "tX_test_poly  = expand_X(tX_test,d)\n",
    "tX_test_poly[:,1:]  = (tX_test_poly[:,1:]-mu_train_poly)/std_train_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_analytical, tX_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
