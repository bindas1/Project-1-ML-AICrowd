{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions 1 is \"s\", -1 is \"b\"\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tX1:\n",
      "number of features: 18\n",
      "tX2:\n",
      "number of features: 22\n",
      "tX3\n",
      "number of features: 29\n",
      "tX4\n",
      "number of features: 29\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "from clean_data_4_categories import *\n",
    "\n",
    "# save ind_delete to delete indices from test data\n",
    "#tX_clean, ind_delete = update_X(tX, bound_delete=0.9, bound_change=0.05)\n",
    "\n",
    "#Divide the data in 4 categories depending on the value of PRI_jet_num (column 22), and remove all columns where standard variation is 0\n",
    "print(\"tX1:\")\n",
    "tX1_ind = np.array(tX[:, 22]==0.)\n",
    "tX1 = tX[tX[:, 22]==0.]\n",
    "tX1 = clean_data(tX1)\n",
    "y1 = y[tX[:, 22]==0.]\n",
    "print(\"number of features:\", tX1.shape[1])\n",
    "\n",
    "print(\"tX2:\")\n",
    "tX2_ind = np.array(tX[:, 22]==1.)\n",
    "tX2 = tX[tX[:, 22]==1.]\n",
    "tX2 = clean_data(tX2)\n",
    "y2 = y[tX[:, 22]==1.]\n",
    "print(\"number of features:\", tX2.shape[1])\n",
    "\n",
    "print(\"tX3\")\n",
    "tX3_ind = np.array(tX[:, 22]==2.)\n",
    "tX3 = tX[tX[:, 22]==2.]\n",
    "tX3 = clean_data(tX3)\n",
    "y3 = y[tX[:, 22]==2.]\n",
    "print(\"number of features:\", tX3.shape[1])\n",
    "\n",
    "print(\"tX4\")\n",
    "tX4_ind = np.array(tX[:, 22]==3.)\n",
    "tX4 = tX[tX[:, 22]==3.]\n",
    "tX4 = clean_data(tX4)\n",
    "y4 = y[tX[:, 22]==3.]\n",
    "print(\"number of features:\", tX4.shape[1])\n",
    "\n",
    "print(y1.shape[0] + y2.shape[0] + y3.shape[0] + y4.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_expand_data import *\n",
    "\n",
    "# split data into train and test sets\n",
    "X1_train, y1_train, X1_test, y1_test = split_data(tX1, y1, split_ratio=0.7)\n",
    "X2_train, y2_train, X2_test, y2_test = split_data(tX2, y2, split_ratio=0.7)\n",
    "X3_train, y3_train, X3_test, y3_test = split_data(tX3, y3, split_ratio=0.7)\n",
    "X4_train, y4_train, X4_test, y4_test = split_data(tX4, y4, split_ratio=0.7)\n",
    "\n",
    "# store d to later use with real test set\n",
    "d=7\n",
    "\n",
    "# create expanded X_train and X_test and normalize\n",
    "\n",
    "#Category 1\n",
    "X1_train_poly, X1_mu_train_poly, X1_std_train_poly = expand_and_normalize_X(X1_train, d)\n",
    "X1_test_poly  = expand_X_cross_1_trigo(X1_test, d, 2)\n",
    "X1_test_poly[:,1:] = (X1_test_poly[:,1:] - X1_mu_train_poly) / X1_std_train_poly\n",
    "\n",
    "#Category 2\n",
    "X2_train_poly, X2_mu_train_poly, X2_std_train_poly = expand_and_normalize_X(X2_train, d)\n",
    "X2_test_poly  = expand_X_cross_1_trigo(X2_test, d, 2)\n",
    "X2_test_poly[:,1:] = (X2_test_poly[:,1:] - X2_mu_train_poly) / X2_std_train_poly\n",
    "\n",
    "#Category 3\n",
    "X3_train_poly, X3_mu_train_poly, X3_std_train_poly = expand_and_normalize_X(X3_train, d)\n",
    "X3_test_poly  = expand_X_cross_1_trigo(X3_test, d, 2)\n",
    "X3_test_poly[:,1:] = (X3_test_poly[:,1:] - X3_mu_train_poly) / X3_std_train_poly\n",
    "\n",
    "#Category 4\n",
    "X4_train_poly, X4_mu_train_poly, X4_std_train_poly = expand_and_normalize_X(X4_train, d)\n",
    "X4_test_poly  = expand_X_cross_1_trigo(X4_test, d, 2)\n",
    "X4_test_poly[:,1:] = (X4_test_poly[:,1:] - X4_mu_train_poly) / X4_std_train_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Least Squares GD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27469350541646415\n",
      "0.36255526428188595\n",
      "0.3611027320775547\n",
      "0.3646255985397779\n"
     ]
    }
   ],
   "source": [
    "from least_squares_GD import *\n",
    "\n",
    "\n",
    "max_iters = 300\n",
    "gamma = 0.008\n",
    "\n",
    "initial_w1 = np.random.normal(0, 1e-1, X1_train_poly.shape[1])\n",
    "w1, loss1 = least_squares_GD(y1_train, X1_train_poly, initial_w1, max_iters, gamma)\n",
    "print(loss1)\n",
    "\n",
    "initial_w2 = np.random.normal(0, 1e-1, X2_train_poly.shape[1])\n",
    "w2, loss2 = least_squares_GD(y2_train, X2_train_poly, initial_w2, max_iters, gamma)\n",
    "print(loss2)\n",
    "\n",
    "initial_w3 = np.random.normal(0, 1e-1, X3_train_poly.shape[1])\n",
    "w3, loss3 = least_squares_GD(y3_train, X3_train_poly, initial_w3, max_iters, gamma)\n",
    "print(loss3)\n",
    "\n",
    "initial_w4 = np.random.normal(0, 1e-1, X4_train_poly.shape[1])\n",
    "w4, loss4 = least_squares_GD(y4_train, X4_train_poly, initial_w4, max_iters, gamma)\n",
    "print(loss4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crooss validation and bias variance playing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use expand X its same\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    pm = np.ones((x.shape[0],degree+1))\n",
    "    for i in range(degree+1): pm[:,i] = x**i \n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import compute_loss_mse\n",
    "from least_squares import *\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_test = y[te_indice]\n",
    "    y_train = y[tr_indice]\n",
    "    x_test = x[te_indice]\n",
    "    x_train = x[tr_indice]\n",
    "    \n",
    "    x_train_poly = build_poly(x_train, degree)\n",
    "    x_test_poly = build_poly(x_test, degree)\n",
    "\n",
    "    w = least_squares(y_train, x_train_poly, lambda_)[0]\n",
    "    \n",
    "    loss_tr = compute_loss_mse(y_train, x_train_poly, w)\n",
    "    loss_te = compute_loss_mse(y_test, x_test_poly, w)\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this can be used to validate our lambdas\n",
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_tr.T,\n",
    "        'b',\n",
    "        linestyle=\"-\",\n",
    "        color=([0.7, 0.7, 1]),\n",
    "        label='train',\n",
    "        linewidth=0.3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_te.T,\n",
    "        'r',\n",
    "        linestyle=\"-\",\n",
    "        color=[1, 0.7, 0.7],\n",
    "        label='test',\n",
    "        linewidth=0.3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_tr_mean.T,\n",
    "        'b',\n",
    "        linestyle=\"-\",\n",
    "        label='train',\n",
    "        linewidth=3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_te_mean.T,\n",
    "        'r',\n",
    "        linestyle=\"-\",\n",
    "        label='test',\n",
    "        linewidth=3)\n",
    "    plt.ylim(0.2, 0.7)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n",
    "    \n",
    "    # COMMENT THIS FOR NO Y LOG SCALE\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-17f72567f973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbias_variance_decomposition_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbias_variance_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-17f72567f973>\u001b[0m in \u001b[0;36mbias_variance_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mratio_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_train_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_and_normalize_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tX_clean' is not defined"
     ]
    }
   ],
   "source": [
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(5)\n",
    "    ratio_train = 0.7\n",
    "    degrees = range(1, 10)\n",
    "    \n",
    "    # define list to store the variable\n",
    "    mse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    mse_te = np.empty((len(seeds), len(degrees)))\n",
    "    \n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        x_train, y_train, x_test, y_test = split_data(tX_clean, y, split_ratio=ratio_train, seed=seed)\n",
    "        for i, d in enumerate(degrees):\n",
    "            X_train_poly, mu_train_poly, std_train_poly = expand_and_normalize_X(X_train,d)\n",
    "            X_test_poly  = expand_X(X_test,d)\n",
    "            X_test_poly[:,1:]  = (X_test_poly[:,1:]-mu_train_poly)/std_train_poly\n",
    "            w = least_squares(y_train, X_train_poly)[0]\n",
    "            mse_tr[index_seed, i] = compute_loss_mse(y_train, X_train_poly, w)\n",
    "            mse_te[index_seed, i] = compute_loss_mse(y_test, X_test_poly, w)\n",
    "\n",
    "    bias_variance_decomposition_visualization(degrees, mse_tr, mse_te)\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2327024807319274\n",
      "7.816363871906081e+18\n",
      "0.30172466687292643\n",
      "2.1173065478484294\n",
      "0.2641333709548907\n",
      "3.065267608282525\n",
      "0.27070210461447897\n",
      "0.5774295415702042\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "\n",
    "w1, loss_analytical1 = least_squares(y1_train, X1_train_poly)\n",
    "print(loss_analytical1)\n",
    "print(compute_loss_mse(y1_test, X1_test_poly, w1))\n",
    "\n",
    "w2, loss_analytical2 = least_squares(y2_train, X2_train_poly)\n",
    "print(loss_analytical2)\n",
    "print(compute_loss_mse(y2_test, X2_test_poly, w2))\n",
    "\n",
    "w3, loss_analytical3 = least_squares(y3_train, X3_train_poly)\n",
    "print(loss_analytical3)\n",
    "print(compute_loss_mse(y3_test, X3_test_poly, w3))\n",
    "\n",
    "w4, loss_analytical4 = least_squares(y4_train, X4_train_poly)\n",
    "print(loss_analytical4)\n",
    "print(compute_loss_mse(y4_test, X4_test_poly, w4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions based on w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers\n",
    "\n",
    "p1 = predict_labels(w1, X1_test_poly)\n",
    "p2 = predict_labels(w2, X2_test_poly)\n",
    "p3 = predict_labels(w3, X3_test_poly)\n",
    "p4 = predict_labels(w4, X4_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Category 1: 84.7 %\n",
      "Train Accuracy Category 2: 79.2 %\n",
      "Train Accuracy Category 3: 82.2 %\n",
      "Train Accuracy Category 4: 82.1 %\n",
      "82.26847284072424\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = (np.mean(p1 == y1_test) * 100)\n",
    "print('Train Accuracy Category 1: %.1f %%' % accuracy1)\n",
    "accuracy2 = (np.mean(p2 == y2_test) * 100)\n",
    "print('Train Accuracy Category 2: %.1f %%' % accuracy2)\n",
    "accuracy3 = (np.mean(p3 == y3_test) * 100)\n",
    "print('Train Accuracy Category 3: %.1f %%' % accuracy3)\n",
    "accuracy4 = (np.mean(p4 == y4_test) * 100)\n",
    "print('Train Accuracy Category 4: %.1f %%' % accuracy4)\n",
    "print(np.sum([len(y1_test)*accuracy1, len(y2_test)*accuracy2, len(y3_test)*accuracy3, len(y4_test)*accuracy4])/(len(y1_test)+len(y2_test)+len(y3_test)+len(y4_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../data/test.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-ba835f65adc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_TEST_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/test.csv'\u001b[0m \u001b[0;31m# TODO: download test data and supply path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ML-Projects/Project-1-ML-AICrowd/src/proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(data_path, sub_sample)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ../data/test.csv not found."
     ]
    }
   ],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column : 4\n",
      "Column : 5\n",
      "Column : 6\n",
      "Column : 12\n",
      "Column : 22\n",
      "Column : 23\n",
      "Column : 24\n",
      "Column : 25\n",
      "Column : 26\n",
      "Column : 27\n",
      "Column : 28\n",
      "Column : 29\n",
      "Column : 4\n",
      "Column : 5\n",
      "Column : 6\n",
      "Column : 12\n",
      "Column : 22\n",
      "Column : 26\n",
      "Column : 27\n",
      "Column : 28\n",
      "Column : 22\n",
      "Column : 22\n"
     ]
    }
   ],
   "source": [
    "tX1_test_ind = np.array(tX_test[:, 22]==0.)\n",
    "tX1_test = tX_test[tX_test[:, 22]==0.]\n",
    "tX1_test = clean_data(tX1_test)\n",
    "\n",
    "tX2_test_ind = np.array(tX_test[:, 22]==1.)\n",
    "tX2_test = tX_test[tX_test[:, 22]==1.]\n",
    "tX2_test = clean_data(tX2_test)\n",
    "\n",
    "tX3_test_ind = np.array(tX_test[:, 22]==2.)\n",
    "tX3_test = tX_test[tX_test[:, 22]==2.]\n",
    "tX3_test = clean_data(tX3_test)\n",
    "\n",
    "tX4_test_ind = np.array(tX_test[:, 22]==3.)\n",
    "tX4_test = tX_test[tX_test[:, 22]==3.]\n",
    "tX4_test = clean_data(tX4_test)\n",
    "\n",
    "\n",
    "#Category 1\n",
    "tX1_test_poly  = expand_X_cross_1_trigo(tX1_test, d, 2)\n",
    "tX1_test_poly[:,1:] = (tX1_test_poly[:,1:] - X1_mu_train_poly) / X1_std_train_poly\n",
    "\n",
    "#Category 2\n",
    "tX2_test_poly  = expand_X_cross_1_trigo(tX2_test, d, 2)\n",
    "tX2_test_poly[:,1:] = (tX2_test_poly[:,1:] - X2_mu_train_poly) / X2_std_train_poly\n",
    "\n",
    "#Category 3\n",
    "tX3_test_poly  = expand_X_cross_1_trigo(tX3_test, d, 2)\n",
    "tX3_test_poly[:,1:] = (tX3_test_poly[:,1:] - X3_mu_train_poly) / X3_std_train_poly\n",
    "\n",
    "#Category 4\n",
    "tX4_test_poly  = expand_X_cross_1_trigo(tX4_test, d, 2)\n",
    "tX4_test_poly[:,1:] = (tX4_test_poly[:,1:] - X4_mu_train_poly) / X4_std_train_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "y_pred = np.empty(tX_test.shape[0])\n",
    "y1_pred = predict_labels(w1, tX1_test_poly)\n",
    "y2_pred = predict_labels(w2, tX2_test_poly)\n",
    "y3_pred = predict_labels(w3, tX3_test_poly)\n",
    "y4_pred = predict_labels(w4, tX4_test_poly)\n",
    "\n",
    "y_pred[tX1_test_ind] = y1_pred\n",
    "y_pred[tX2_test_ind] = y2_pred\n",
    "y_pred[tX3_test_ind] = y3_pred\n",
    "y_pred[tX4_test_ind] = y4_pred\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
